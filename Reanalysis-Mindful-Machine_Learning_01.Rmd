---
title: "Re-analysis of the paper 'Mindful machine learning: Using machine learning algorithms to predict the practice of mindfulness'"
author: "Sebastian Sauer"
date: "`r Sys.Date()`"
output: 
  html_document: 
    df_print: kable
    toc: true
---


# Scope

This is the re-analysis of this paper

Sauer, S., Buettner, R., Heidenreich, T., Lemke, J., Berg, C., & Kurz, C. (2018). 
Mindful machine learning: Using machine learning algorithms to predict the practice of mindfulness. 
European Journal of Psychological Assessment, 34(1), 6-13.
http://dx.doi.org/10.1027/1015-5759/a000312



# Motive


Dr. Florian Pargent contacted me, being the corresponing author of the above mentioned paper, and informed me that he found anerror in our analysis. 
Specifically, he informed us that the performance measures appear to be based on on the train sample, 
where the test sample should have been used.
I agree that such an error, given that it exists, needs correction and warrants investigation.
In addition, the argument and the analyses provided by Dr. Pargent seems plausible.


# Course of analysis

After having checked the original code, I agree with Dr. Pargent notion. Specifically, in *each model*, the *training* data has been presented, but not the *testing* data as it should have been.

The following code documents the re-analyses that now employs the *testing* data for gauging model performance.


# Cautions

The following aspects could not been controlled:

- Package versions
- R version


It seems unlikely though not completely impossible that these factors possibly influence the results. However, we deem the possible influence of those factor negligible.



# Accessibility


This code, alongside with the data, can be openly accessed from this repository: https://github.com/sebastiansauer/reanalysis-mindful-machine-learning



# Setup


## Init

```{r knitr-setup}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = TRUE,
  results = "hide",
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold",
  size = "tiny"
)
```

Load libs:

```{r libs, message = FALSE}
#library(conflicted)
library(caret)
library(here)
library(tictoc)
library(testthat)
library(tidyverse)
library(drlib)
```


## Define constants


```{r}
output_filename <- "model_performance.Rds"
save_to_disk <- FALSE
output_path <- paste0(here::here(),"/reanalysis-objects/")
output_filename_nested <- "model_performance_nested.rds"
recompute_it <- FALSE
```




## Load original data



```{r read-data}
dat_url <- "https://raw.github.com/sebastiansauer/reanalysis-mindful-machine-learning/master/original-data/1015-5759_a000312_esm5.csv"

dat <- read_csv(dat_url)
```







## Prepare data



### Set DV to type `factor`

As `caret` will only do classificiton if the outcome is of type factor, we need to convert the outcome variable `practice` to factor.

```{r prep, results = "show"}
dat2 <- dat %>% 
  mutate(practice = factor(practice),
         practice = case_when(
           practice == "1" ~ "yes",
           practice == "0" ~ "no"
         ),
         practice = factor(practice)) %>% 
  dplyr::select(practice, everything()) %>% 
  dplyr::select(-X1)

glimpse(dat2)
```



### Ensure valide factor levels

The factor levels should be proper R names for some caret models, accoring to [this source](https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015/discussion/13964), see also [this SO post](https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret).


The dependent variable should be of type `factor` and needs valid factor levels [as stated in this SO post](https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret).
Let's check that.


```{r results = "show"}
levels(dat2$practice) <- make.names(levels(factor(dat2$practice)))
levels(dat2$practice)
```













Check that all predictors are metric, and that the DV is of type factor.

Define vector of predictor names:

```{r}
predictor_names <- c(paste0("fmi", 1:14))
predictor_names
```



Test for correct length:

```{r}
test_dat2 <- dat2 %>% 
  map( ~class(.))

expect_length(test_dat2, n = (length(predictor_names) + 1))
```






## Training vs test sample

```{r train-test-sample}
set.seed(42)
inTraining <- createDataPartition(dat2$practice, p = 0.8, list = FALSE)
training <- dat2[inTraining, ]
testing <- dat2[-inTraining, ]
```






## Define model list

```{r model-list}
model_list <- c("glm", "gbm", "qda", "svmLinear", "svmPoly", "rf", "nnet", "ada", "knn", "elm")
model_list <- set_names(model_list, paste0("model", 1:length(model_list)))
model_list

n_models <- length(model_list)
n_models
```




There are `r n_models` being tested.


## Define standard cross-validation scheme


```{r fitControl}
fitControl <- trainControl(## 10-fold CV
    method = "repeatedcv",
    number = 10,
    ## repeated ten times
    repeats = 10,
    savePredictions = "all")
```



## Define data objects for results


```{r}
model_output <- list()
```


# funs


```{r run-model-fun}
run_model <- function(model_name, 
                      training = training, 
                      testing = testing,...){

  set.seed(42)

  start_time = Sys.time()

  modelOut <- train(practice ~ .,
                    data = training,
                    method = model_name,
                    trControl = fitControl,
                    ...)

  modelPred <- predict(modelOut, testing)

  modelConfMatrix <- confusionMatrix(modelPred, testing$practice)

  end_time = Sys.time()

  coefs <- list(name = model_name,
                sensitivity = modelConfMatrix$byClass[1],
                specificity = modelConfMatrix$byClass[2],
                accuracy = modelConfMatrix$overall[1],
                kappa = modelConfMatrix$overall[2],
                time_taken = end_time - start_time)
}
```


# Now run the models


## demo run


```{r check-run-model, results = "hide"}
modelOut <- train(practice ~ .,
                  data = training,
                  method = "rf",
                  trControl = fitControl)

modelPred <- predict(modelOut, testing)
```

Show check results:

```{r}
modelConfMatrix <- confusionMatrix(modelPred, testing$practice)
modelConfMatrix


modelConfMatrix$byClass[c(1,2)]

modelConfMatrix$overall[c(1,2)]
```






## Check demo run


```{r results = "hide", message = "false"}
dummy <- run_model("gbm")
str(dummy)
```



### Candidate model results

Overview:

```{r}
modelOut
```



Aggregated results for each candidate model (ie., for each hyper parameter configuration):

Note that for each value of mtry, 100 Resamples have been drawn.

```{r}
modelOut$results
```


As can be seen, the following hyper parameter vales have been tried:

- mtry
  - 2
  - 8
  - 14

  
  
3 values, that is yielding 3 candidate models.


From the caret [documentation](https://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning):

>    By default, if p is the number of tuning parameters, the grid size is 3^p



### Resampling profile


```{r}
plot(modelOut)
```




```{r}
trellis.par.set(caretTheme())
plot(modelOut, metric = "Kappa", plotType = "level",
     scales = list(x = list(rot = 90)))
```




### Best candidate model


The best candidate model (based on the training data) is this one:


```{r}
modelOut$bestTune
```




### Details for best candidate model

mtry=2 was the best candidate in the training process.

These are the 100 resamples for this candidate:

```{r}
modelOut$resample %>% glimpse()


modelOut$resample %>% head() %>% knitr::kable()
```



Hence the mean accuracy and the mean kappa are:

```{r}
modelOut$resample %>% 
  summarise_at(vars(Accuracy, Kappa), list(mean))
```

These are the values reported above.


### Predictions


```{r}
train_pred <- modelOut$pred

train_pred %>% head()
```


How many resamples repetitions? How many candidate models?

```{r}
train_pred %>% 
  group_by(mtry, Resample) %>% 
  summarise(n = n())
```


There are 300 samples being predicted:

- 3 parameters
- 100 Resample each

yielding 300 samples.













## Loop over models




```{r}
if (!file.exists(paste0(here(),"/reanalysis-objects/", output_filename))) {
  time <- Sys.time()
  model_performance <- model_list %>% 
    map_dfr(run_model)
  
  Sys.time() - time
} else {cat("Output file already exists. Skipping re-computation. Loading output file.\n")
  model_performance <- read_rds(paste0(here(),"/reanalysis-objects/", output_filename) )}
```




## Check results


```{r}
model_performance
```



## Own music for Boosting

Define different grid to check hyper parameter:

```{r boosting, verbose = FALSE, results = "hide"}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = 1:30*50,
                        shrinkage = 0.1,
                        n.minobsinnode = 10)

performance_gbm <- run_model("gbm", tuneGrid = gbmGrid)


performance_gbm_df <- flatten(performance_gbm) %>% 
  as_tibble()

```


Now add the gbm results to the main performance data frame, that is, replace the old values by the new one:


```{r}
model_performance2 <- model_performance


model_performance2[performance_gbm_df$name == model_performance2$name, ] <- performance_gbm_df


model_performance2
```




# Visualize model performance


## Reformat data

Now spread it into a long format.

```{r output-long}
model_performance_long <- model_performance2 %>% 
  pivot_longer(cols = sensitivity:kappa, names_to = "coefficient", values_to = "value")
model_performance_long
```


## Check

```{r}
model_performance_long %>% 
  count(name)
```


Looks good, 4 coefficients per model (name).


Similarly,

```{r check-coeffs}
model_performance_long %>% 
  count(coefficient)
```



## Figure 2 (original paper)

Now plot:

```{r figure2, outwidth = "100%"}
model_performance_long %>%
  #dplyr::filter(coefficient != "accuracy") %>% 
  ggplot(aes(x = reorder_within(name, value, coefficient), y = value, fill = coefficient)) +
  geom_col(position = "dodge") +
  coord_flip() +
  scale_fill_viridis_d() +
  scale_x_reordered() +
  theme_light() +
  labs(x = "model", y = "value") +
  facet_wrap(~ coefficient, scales = "free")
```



In the original mode, `rf` scores best. In this re-analysis, `rf` scored third-best (measured by Cohen's Kappa in both analyses).


## Figure 3

Figure 3 in the original paper computes the average per coefficient across all models, and compares "classical" linear model results with the "machine learning" models.

First compute the summary stats.

```{r model-perform-sum}
model_output_df_sum <- model_performance_long %>% 
  mutate(type = ifelse(name == "glm", "classical", "machine learning")) %>% 
  group_by(type, coefficient) %>% 
  summarise(cofficient_mean = mean(value))

model_output_df_sum
```


Now plot.


```{r figure3, outwidth = "100%"}
model_output_df_sum %>% 
  ggplot(aes(x = reorder(coefficient, -cofficient_mean), y = cofficient_mean)) +
  geom_col(position = "dodge", aes(fill = coefficient)) +
  facet_wrap(~ type) +
  scale_fill_viridis_d() +
  theme_light() +
  labs(y = "value", x = "coefficient") +
  geom_label(aes(label = round(cofficient_mean, 2)))
```


There is no strong difference between classical and machine learning models, seen from an summary perspective. However, a slight advantage may be inferred in favor of the ML models. In particular, the best scoring model came from the ML fraction. 

As concluded in the original paper, algorithms do differ in their performance given a particular data set. Analysts should be aware of the variation in the results induced by picking one ore more certain models. 



# Computation time


```{r}
model_performance2 %>%
  mutate(time = as.numeric(time_taken)) %>% 
  ggplot(aes(x = reorder(name, time), y = time)) +
  geom_point(size = 4) +
  coord_flip() +
  theme_light() +
  scale_y_log10() +
  labs(y = "time in log10 [sec]", x = "model")
```


# Save output


```{r save-output}
#model_output <- read_rds("reanalysis-objects/model_output.rds")

if (save_to_disk) {
  write_rds(model_performance, paste0(here(),"/reanalysis-objects/", output_filename))
  write_rds(modelOut, paste0(here(),"/reanalysis-objects/modelOut.rds"))
  cat("Output data has been saved to disk.\n")
  cat(paste0("Path: ", paste0(here(), "/reanalysis-objects/\n")))
}
```





# NESTED RESAMPLING


## Rationale

In this nested cross validation resampling approach, I will put an "outer loop" to the procedure above.


That is, the procedure is as follows:


1. Define number of subsamples (n folds times r repeats) for cross validation 
2. Create folds accoring to previous step
3. For each of the $n*r$ subsamples do
    1. for each model m (rf, gbm, ...) do
        1. Find optimal hyperparameters in training set
        2. predict outcomes in assessment set (within that fold)
3. Aggregate predictions




## (Re)Write function to compute model and give back performance 


```{r fun-run-model-on-subsample }
run_model_on_subsample <- function(trainsample, testsample, modelname, traincontrol, outcomevariable, ...) {
  
  # this functions trains the models from `modellist` on the dataset `trainsample` 
  # using the resampling scheme `traincontrol`
  # `testsample` is used for testing
  # the outcome variable needs be specified too as `outcomevariable`
  # output: one value for (each coefficient of) predictive accuracy for each model
  
  
  
  # make sure the outcome variable has the correct name:
  
  newoutcomevarname <- "outcome"
  trainsample <- dplyr::rename(trainsample, {{newoutcomevarname}} := {{outcomevariable}})
  
  
  set.seed(42)
  
  start_time = Sys.time()

  cat("Starting training.\n")
  cat(paste0("Model name is: ", modelname," \n"))
  
  modelOut <- train(outcome ~ ., 
                    data = trainsample, 
                    method = modelname, 
                    trControl = traincontrol,
                    ...)
  
  modelPred <- predict(modelOut, testing)  # use *test* sample for model performance/predciton
  
  modelConfMatrix <- confusionMatrix(modelPred, testing$practice)
  
  end_time = Sys.time()
  
  coefs <- list(name = modelname, 
                sensitivity = modelConfMatrix$byClass[1], 
                specificity = modelConfMatrix$byClass[2], 
                accuracy = modelConfMatrix$overall[1],
                kappa = modelConfMatrix$overall[2],
                time_taken = end_time - start_time)
  
  return(coefs)
  cat("Model results have been returned.\n")


}

```



### Check

```{r results = "show"}
dummy <- run_model_on_subsample(modelname = "gbm", 
                                trainsample = training, 
                                testsample = testing, 
                                traincontrol = fitControl,
                                outcomevariable = "practice")
dummy
```


## Write function for inner loop


This function runs `run_model_on_subsample` on each model from `model_list`. It's basically a loop.





```{r fun-run-modellist}
run_modellist <- function(modellist, trainsample, testsample, traincontrol, outcomevariable, outputfilename = NA, ...){

if (is.na(outputfilename)) {
  time <- Sys.time()
  model_performance <- modellist %>% 
    map_dfr( ~ run_model_on_subsample(trainsample = trainsample,
                                      testsample = testsample,
                                      traincontrol = traincontrol,
                                      outcomevariable = outcomevariable, ...))
  
  Sys.time() - time
  
  return(model_performance)
  
  
  
} else {cat("Output file already exists. Skipping re-computation. Loading output file.\n")
  model_performance <- savely(read_rds(output_filename))}
  
  
}
```



### Check

```{r}

models <- list(model1 = "glm",
               model2 = "gbm")

dummy <- models %>% 
  map_dfr( ~ run_model_on_subsample(trainsample = training,
                                      testsample = testing,
                                      traincontrol = fitControl,
                                      outcomevariable = "practice"))
dummy


#undebug(run_modellist)
dummy <- run_modellist(modellist = models,
                       trainsample = training,
                       testsample = testing,
                       traincontrol = fitControl,
                       outcomevariable = "practice")
dummy



```





## Define subsamples



### Define number of subsamples

Take the *whole* dataset and create subsamples


```{r}
t <- 3  # times repeated
k <- 5  # number of folds
```

The whole number of subsmaples is $t * k$.

### Create subsamples

Now create the subsamples

```{r}
subsamples <- createMultiFolds(dat2$practice, k = k, times = t) ## t rep k-fold CV for performance evaluation
```

How many subsamples in total?

```{r}
length(subsamples)
```


### Check

How large is one fold and the corrsponding test set?


```{r}
check_train <- dat2[subsamples[[1]], ] 
check_test <- dat2[-subsamples[[1]], ]

nrow(check_train)
nrow(check_test)
```


By definition/by virtue of this method, the two check samples (check_train/check_test) will consist of completely cases, the union set will be empty.



## Outer Loop function


1. For each subsample
    1. do the inner loop




```{r fun-run-models-on-all-samples}
run_models_on_all_subsamples <- function(data, 
                                         subsamples, 
                                         modellist, 
                                         traincontrol, 
                                         outcomevariable, ...) {
  
  # this functions computes the specified models in `modellist` on all `subsamplies` 
  # output: list of length according to the number of subsamples. 
  #   Each list element is a df with the performance indicators, 
  #   where a row is a model and a col is a coefficient.
  
  cat(paste0("Number of subsamples to be processed: ", length(subsamples), " .\n"))
  cat(paste0("Model list: ", modellist, ".\n"))
  
  for (i in 1:length(subsamples)) {
    
    cat(paste0("subsample: ",i," \n"))
    
    out[[i]] <- run_modellist(modellist = modellist,
                              trainsample = data[subsamples[[i]], ],
                              testsample = data[-subsamples[[i]], ],
                              traincontrol = traincontrol,
                              outcomevariable = outcomevariable, ...)
    
    out[[i]] <- out[[i]] %>% 
      mutate(subsample = names(subsamples)[[i]])
    
  }
  
   return(out)
}

```




## Run the whole shebang


```{r run-all-samples, cache = TRUE, results = "hide", message = "FALSE"}


if (!file.exists(paste0(output_path, output_filename_nested)) || recompute_it == TRUE){
  
  cat("Starting the main computation\n")
  
  out <- list()
  out <- run_models_on_all_subsamples(data = dat2,
                                      subsamples = subsamples,
                                      modellist = model_list,
                                      traincontrol = fitControl,
                                      outcomevariable = "practice")
} else {
  cat("Output file for nested corss validation exists. I'll just load it.\n")
  out <- read_rds(paste0(output_path, output_filename_nested))
}



```




## Reformat list to df


```{r list-to-dfr}
out_df <- out %>% 
  map_dfr( ~ .) 
```





## Write to disk


```{r write-main-out-to-disk}
if (save_to_disk == TRUE){
  write_rds(out, path = paste0(output_path, output_filename_nested))
  write_rds(out_df, path = paste0(output_path, "model_performance_nested_df.rds"))
}
```



## Run models with alternative search grid (gbm)

In the original papeer, GBM uses its own search grid for hyperparamters, so we run the main function for this model again:


```{r again-for-fbm}
out_gbm <- run_models_on_all_subsamples(data = dat2,
                                        subsamples = subsamples,
                                        modellist = "gbm",
                                        traincontrol = fitControl,
                                        outcomevariable = "practice",
                                        tuneGrid = gbmGrid)
```




```{r list-to-dfr}
out_gbm_df <- out_gbm %>% 
  map_dfr( ~ .) 


out_gbm_df
```



## Replace gbm with correct tuning grid runs

```{r}
out2 <- out_df

out2[out2$name == "gbm", ] <- out_gbm_df

out2 %>% 
  filter(name == "gbm")
```








# Visualize model performance of NESTED resampling scheme


## Reformat data

Now spread it into a long format.

```{r output-long}
model_performance2_long <- out2 %>% 
  pivot_longer(cols = sensitivity:kappa, names_to = "coefficient", values_to = "value")
model_performance2_long
```

10 models by 15 subsamples by 4 coefficeints equals 600.


## Check

```{r}
model_performance2_long %>% 
  count(name)
```


Looks good, 60 rows, ie 15 subsamples by 4 coefficients per model.


Similarly,

```{r check-coeffs}
model_performance2_long %>% 
  count(coefficient)
```



## Figure 2 (original paper)

Now plot:

```{r figure2, outwidth = "100%"}
model_performance2_long %>%
  #dplyr::filter(coefficient != "accuracy") %>% 
  ggplot(aes(x = reorder_within(name, value, coefficient), y = value)) +
  geom_jitter(aes(color = name)) +
  coord_flip() +
  scale_fill_viridis_d() +
  scale_x_reordered() +
  theme_light() +
  labs(x = "model", y = "value", coefficient = "", color = "model name") +
  facet_wrap(~ coefficient, scales = "free")
```



In the original mode, `rf` scores best. In this re-analysis, `rf` scored third-best (measured by Cohen's Kappa in both analyses).


## Figure 3

Figure 3 in the original paper computes the average per coefficient across all models, and compares "classical" linear model results with the "machine learning" models.

First compute the summary stats.

```{r model-perform-sum}
model_output_df_sum <- model_performance_long %>% 
  mutate(type = ifelse(name == "glm", "classical", "machine learning")) %>% 
  group_by(type, coefficient) %>% 
  summarise(cofficient_mean = mean(value))

model_output_df_sum
```


Now plot.


```{r figure3, outwidth = "100%"}
model_output_df_sum %>% 
  ggplot(aes(x = reorder(coefficient, -cofficient_mean), y = cofficient_mean)) +
  geom_col(position = "dodge", aes(fill = coefficient)) +
  facet_wrap(~ type) +
  scale_fill_viridis_d() +
  theme_light() +
  labs(y = "value", x = "coefficient") +
  geom_label(aes(label = round(cofficient_mean, 2)))
```


There is no strong difference between classical and machine learning models, seen from an summary perspective. However, a slight advantage may be inferred in favor of the ML models. In particular, the best scoring model came from the ML fraction. 

As concluded in the original paper, algorithms do differ in their performance given a particular data set. Analysts should be aware of the variation in the results induced by picking one ore more certain models. 



# Computation time


```{r}
model_performance2 %>%
  mutate(time = as.numeric(time_taken)) %>% 
  ggplot(aes(x = reorder(name, time), y = time)) +
  geom_point(size = 4) +
  coord_flip() +
  theme_light() +
  scale_y_log10() +
  labs(y = "time in log10 [sec]", x = "model")
```
